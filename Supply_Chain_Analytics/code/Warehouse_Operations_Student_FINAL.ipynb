{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdc2b852",
   "metadata": {},
   "source": [
    "# Warehouse Operations\n",
    "\n",
    "This notebook is intentionally structured as a **guided analytics lab**.\n",
    "\n",
    "Scope  \n",
    "Warehouse execution only (order flow, labor productivity, congestion risk)\n",
    "\n",
    "Analytics Flow  \n",
    "Descriptive → Diagnostic → Predictive → Prescriptive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e043a183",
   "metadata": {},
   "source": [
    "## Hands-On Table (Follow in Order)\n",
    "\n",
    "| Step | What | Cell ID | What to do in Colab | Why (Purpose) |\n",
    "|---|---|---|---|---|\n",
    "| 0A | Load and inspect dataset | `0A` | Run the load cell and inspect `df.head()` and `df.columns` | Understand execution-level fields |\n",
    "| 0B | Identify variable roles | `0B` | Fill STATE / FLOW / CONSTRAINT lists | Build correct system mental model |\n",
    "| 1A | Construct warehouse cycle time | `1A` | Set `CYCLE_START` and `CYCLE_END` to correct timestamps | Measure execution speed (Order→Ship) |\n",
    "| 1B | Daily KPI baseline | `1B` | Run daily aggregation and read KPI outputs | Establish baseline behavior |\n",
    "|1C|Baseline dashboard charts| `1C` |Plot inventory, throughput, cycle-time plots|Make warehouse KPIs observable|\n",
    "| 2A | Construct pick labor-hours + pick rate | `2A` | Set `PICK_START` and `PICK_END`, compute `pick_labor_hrs` and `pick_rate` | Build capacity logic from work time |\n",
    "| 2B | Compute utilization + WIP proxy | `2B` | Define `arrival_rate` and `capacity_rate`, compute utilization & WIP | Diagnose congestion mechanisms |\n",
    "| 3A | Predictive: demand shock | `3A` | Set `DEMAND_MULTIPLIER` and recompute utilization | Predict stress response |\n",
    "| 3B | Diagnostic check: scaling pattern | `3B` | Compare baseline vs shocked utilization patterns | Validate linear scaling under fixed capacity |\n",
    "| 4A | Prescriptive: target utilization | `4A` | Set `TARGET_UTIL` (e.g., 0.85) | Frame decision target |\n",
    "| 4B | Prescriptive: required staffing | `4B` | Compute required workers and staffing gap | Translate analytics into action |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20f50a8",
   "metadata": {},
   "source": [
    "## Step 0A — Load and Inspect Dataset\n",
    "\n",
    "**Cell ID:** `0A`\n",
    "\n",
    "Run the cell. Then scan the column names and 3–5 sample rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b98865d",
   "metadata": {
    "tags": [
     "0A"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order_ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Week_Num</th>\n",
       "      <th>SKU</th>\n",
       "      <th>Zone</th>\n",
       "      <th>Order_Time</th>\n",
       "      <th>Pick_Start</th>\n",
       "      <th>Pick_End</th>\n",
       "      <th>Ship_Time</th>\n",
       "      <th>Picks</th>\n",
       "      <th>Workers_On_Shift</th>\n",
       "      <th>Shift_Hours</th>\n",
       "      <th>Demand</th>\n",
       "      <th>Receipt_Qty</th>\n",
       "      <th>On_Hand_Inventory</th>\n",
       "      <th>Backorder_Qty</th>\n",
       "      <th>Is_Promo</th>\n",
       "      <th>Price</th>\n",
       "      <th>On_Order_Qty</th>\n",
       "      <th>Lead_Time_Days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WO000001</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>SKU-INV-001</td>\n",
       "      <td>A</td>\n",
       "      <td>2024-01-01 12:04:34.371709988</td>\n",
       "      <td>2024-01-01 12:22:04.707434858</td>\n",
       "      <td>2024-01-01 12:27:23.456944007</td>\n",
       "      <td>2024-01-01 12:47:37.758316628</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>8.12</td>\n",
       "      <td>23.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WO000002</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>SKU-INV-001</td>\n",
       "      <td>B</td>\n",
       "      <td>2024-01-01 14:31:33.811356105</td>\n",
       "      <td>2024-01-01 14:47:16.663750715</td>\n",
       "      <td>2024-01-01 14:53:13.985610003</td>\n",
       "      <td>2024-01-01 15:28:40.143464557</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>8.12</td>\n",
       "      <td>23.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WO000003</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>SKU-INV-001</td>\n",
       "      <td>A</td>\n",
       "      <td>2024-01-01 09:21:23.120473445</td>\n",
       "      <td>2024-01-01 09:45:44.409387040</td>\n",
       "      <td>2024-01-01 09:48:14.409387040</td>\n",
       "      <td>2024-01-01 10:30:22.181918453</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>8.12</td>\n",
       "      <td>23.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WO000004</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>SKU-INV-001</td>\n",
       "      <td>C</td>\n",
       "      <td>2024-01-01 11:27:53.585366230</td>\n",
       "      <td>2024-01-01 11:34:45.424021159</td>\n",
       "      <td>2024-01-01 11:41:55.565324070</td>\n",
       "      <td>2024-01-01 12:09:04.791376229</td>\n",
       "      <td>27</td>\n",
       "      <td>8</td>\n",
       "      <td>8.12</td>\n",
       "      <td>23.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WO000005</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>SKU-INV-001</td>\n",
       "      <td>A</td>\n",
       "      <td>2024-01-01 13:01:37.628705527</td>\n",
       "      <td>2024-01-01 13:12:16.734844509</td>\n",
       "      <td>2024-01-01 13:17:33.431139791</td>\n",
       "      <td>2024-01-01 13:35:00.961321182</td>\n",
       "      <td>27</td>\n",
       "      <td>8</td>\n",
       "      <td>8.12</td>\n",
       "      <td>23.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Order_ID       Date  Week_Num          SKU Zone                    Order_Time                    Pick_Start                      Pick_End  \\\n",
       "0  WO000001 2024-01-01         1  SKU-INV-001    A 2024-01-01 12:04:34.371709988 2024-01-01 12:22:04.707434858 2024-01-01 12:27:23.456944007   \n",
       "1  WO000002 2024-01-01         1  SKU-INV-001    B 2024-01-01 14:31:33.811356105 2024-01-01 14:47:16.663750715 2024-01-01 14:53:13.985610003   \n",
       "2  WO000003 2024-01-01         1  SKU-INV-001    A 2024-01-01 09:21:23.120473445 2024-01-01 09:45:44.409387040 2024-01-01 09:48:14.409387040   \n",
       "3  WO000004 2024-01-01         1  SKU-INV-001    C 2024-01-01 11:27:53.585366230 2024-01-01 11:34:45.424021159 2024-01-01 11:41:55.565324070   \n",
       "4  WO000005 2024-01-01         1  SKU-INV-001    A 2024-01-01 13:01:37.628705527 2024-01-01 13:12:16.734844509 2024-01-01 13:17:33.431139791   \n",
       "\n",
       "                      Ship_Time  Picks  Workers_On_Shift  Shift_Hours  Demand  Receipt_Qty  On_Hand_Inventory  Backorder_Qty  Is_Promo  Price  On_Order_Qty  \\\n",
       "0 2024-01-01 12:47:37.758316628     40                 8         8.12    23.0         83.0              360.0            0.0       NaN    NaN           0.0   \n",
       "1 2024-01-01 15:28:40.143464557     14                 8         8.12    23.0         83.0              360.0            0.0       NaN    NaN           0.0   \n",
       "2 2024-01-01 10:30:22.181918453     14                 8         8.12    23.0         83.0              360.0            0.0       NaN    NaN           0.0   \n",
       "3 2024-01-01 12:09:04.791376229     27                 8         8.12    23.0         83.0              360.0            0.0       NaN    NaN           0.0   \n",
       "4 2024-01-01 13:35:00.961321182     27                 8         8.12    23.0         83.0              360.0            0.0       NaN    NaN           0.0   \n",
       "\n",
       "   Lead_Time_Days  \n",
       "0             7.0  \n",
       "1             7.0  \n",
       "2             7.0  \n",
       "3             7.0  \n",
       "4             7.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns: ['Order_ID', 'Date', 'Week_Num', 'SKU', 'Zone', 'Order_Time', 'Pick_Start', 'Pick_End', 'Ship_Time', 'Picks', 'Workers_On_Shift', 'Shift_Hours', 'Demand', 'Receipt_Qty', 'On_Hand_Inventory', 'Backorder_Qty', 'Is_Promo', 'Price', 'On_Order_Qty', 'Lead_Time_Days']\n",
      "\n",
      "Rows: 1966\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/saikisri97/17_Hof_Lecture_Code_Pingo/refs/heads/main/Supply_Chain_Analytics/data/warehouse_ops_orders_class_v2.csv\")\n",
    "\n",
    "\n",
    "# Parse timestamps (safe even if already parsed)\n",
    "for c in [\"Date\",\"Order_Time\",\"Pick_Start\",\"Pick_End\",\"Ship_Time\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "\n",
    "display(df.head(5))\n",
    "print(\"\\nColumns:\", list(df.columns))\n",
    "print(\"\\nRows:\", len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663988e1",
   "metadata": {},
   "source": [
    "## Step 0B — Identify Variable Roles\n",
    "\n",
    "**Cell ID:** `0B`\n",
    "\n",
    "Fill lists with column names from the dataset. Keep it simple and defensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f5d7943",
   "metadata": {
    "tags": [
     "0B"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [], [])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Fill lists with column names from df.columns\n",
    "STATE_VARIABLES = []        # quantities describing system state at a point in time\n",
    "FLOW_VARIABLES = []         # quantities describing movement/processing over time\n",
    "CONSTRAINT_VARIABLES = []   # capacity/resources/time constraints\n",
    "\n",
    "STATE_VARIABLES, FLOW_VARIABLES, CONSTRAINT_VARIABLES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb0b39d",
   "metadata": {},
   "source": [
    "## Step 1A — Construct Warehouse Cycle Time (Execution)\n",
    "\n",
    "**Cell ID:** `1A`\n",
    "\n",
    "Choose timestamps that represent warehouse execution lead time (release → ship)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe79c3c5",
   "metadata": {
    "tags": [
     "1A"
    ]
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/argos_env/lib/python3.12/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: None",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m CYCLE_START \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      3\u001b[0m CYCLE_END \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwh_cycle_time_hrs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 6\u001b[0m     pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCYCLE_END\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;241m-\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[CYCLE_START])\n\u001b[1;32m      7\u001b[0m )\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtotal_seconds() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3600\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Basic sanity filter\u001b[39;00m\n\u001b[1;32m     10\u001b[0m df_kpi \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwh_cycle_time_hrs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnotna() \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwh_cycle_time_hrs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/argos_env/lib/python3.12/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/argos_env/lib/python3.12/site-packages/pandas/core/indexes/base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3807\u001b[0m     ):\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: None"
     ]
    }
   ],
   "source": [
    "# TODO: Select correct columns for execution cycle time\n",
    "CYCLE_START = None\n",
    "CYCLE_END = None\n",
    "\n",
    "df[\"wh_cycle_time_hrs\"] = (\n",
    "    pd.to_datetime(df[CYCLE_END]) - pd.to_datetime(df[CYCLE_START])\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "# Basic sanity filter\n",
    "df_kpi = df[df[\"wh_cycle_time_hrs\"].notna() & (df[\"wh_cycle_time_hrs\"] >= 0)].copy()\n",
    "df_kpi[\"wh_cycle_time_hrs\"].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1785e752",
   "metadata": {},
   "source": [
    "## Step 1B — Daily Baseline KPI Summary\n",
    "\n",
    "**Cell ID:** `1B`\n",
    "\n",
    "Run to create a daily table used by all later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4540c11",
   "metadata": {
    "tags": [
     "1B"
    ]
   },
   "outputs": [],
   "source": [
    "df_kpi[\"day\"] = df_kpi[\"Date\"].dt.date\n",
    "\n",
    "daily = df_kpi.groupby(\"day\").agg(\n",
    "    orders=(\"Order_ID\",\"nunique\"),\n",
    "    total_picks=(\"Picks\",\"sum\"),\n",
    "    avg_cycle_time_hrs=(\"wh_cycle_time_hrs\",\"mean\"),\n",
    "    p90_cycle_time_hrs=(\"wh_cycle_time_hrs\", lambda s: np.percentile(s.dropna(), 90)),\n",
    "    workers=(\"Workers_On_Shift\",\"median\"),\n",
    "    shift_hours=(\"Shift_Hours\",\"median\")\n",
    ").reset_index()\n",
    "\n",
    "display(daily.head(10))\n",
    "daily.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcc43a9",
   "metadata": {},
   "source": [
    "## Step 1C — Baseline Dashboard Charts\n",
    "\n",
    "**Cell ID:** `1C`\n",
    "\n",
    "Common warehouse dashboard views: time series + distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34faff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series: avg cycle time\n",
    "plt.figure()\n",
    "plt.plot(pd.to_datetime(daily[\"\"]), daily[\"\"], marker=\"o\")\n",
    "plt.title(\"Avg Warehouse Cycle Time (hrs) — Daily\")\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Hours\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Time series: orders per hour\n",
    "plt.figure()\n",
    "plt.plot(pd.to_datetime(daily[\"\"]), daily[\"\"], marker=\"o\")\n",
    "plt.title(\"Throughput (Orders per Hour) — Daily\")\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Orders/hour\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution: cycle time\n",
    "plt.figure()\n",
    "plt.hist(df_kpi[\"\"].dropna(), bins=30)\n",
    "plt.title(\"Distribution of Warehouse Cycle Time (hrs)\")\n",
    "plt.xlabel(\"Hours\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99935440",
   "metadata": {},
   "source": [
    "## Step 2A — Build Capacity Inputs: Pick Labor-Hours and Pick Rate\n",
    "\n",
    "**Cell ID:** `2A`\n",
    "\n",
    "Pick labor-hours are computed from pick start/end timestamps. Then pick rate = total picks / pick labor-hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129bcfa7",
   "metadata": {
    "tags": [
     "2A"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Select correct pick timestamps (work time spent picking)\n",
    "PICK_START = None\n",
    "PICK_END = None\n",
    "\n",
    "df_kpi[\"pick_labor_hrs\"] = (\n",
    "    pd.to_datetime(df_kpi[PICK_END]) - pd.to_datetime(df_kpi[PICK_START])\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "# keep only valid pick durations\n",
    "df_kpi = df_kpi[df_kpi[\"pick_labor_hrs\"].notna() & (df_kpi[\"pick_labor_hrs\"] >= 0)].copy()\n",
    "\n",
    "pick_daily = df_kpi.groupby(df_kpi[\"day\"]).agg(\n",
    "    pick_labor_hrs=(\"pick_labor_hrs\",\"sum\"),\n",
    "    picks=(\"Picks\",\"sum\")\n",
    ").reset_index()\n",
    "\n",
    "pick_daily[\"pick_rate_picks_per_lh\"] = pick_daily[\"picks\"] / pick_daily[\"pick_labor_hrs\"]\n",
    "\n",
    "display(pick_daily.head(10))\n",
    "pick_daily[\"pick_rate_picks_per_lh\"].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba204dee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "143f31e8",
   "metadata": {},
   "source": [
    "## Step 2B — Utilization and WIP Proxy (Congestion KPIs)\n",
    "\n",
    "**Cell ID:** `2B`\n",
    "\n",
    "Merge pick rate into daily table. Then compute arrival rate, capacity rate, utilization, and WIP proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee41e0f7",
   "metadata": {
    "tags": [
     "2B"
    ]
   },
   "outputs": [],
   "source": [
    "# Merge pick-rate into daily table\n",
    "daily2 = daily.merge(pick_daily[[\"day\",\"pick_labor_hrs\",\"pick_rate_picks_per_lh\"]], on=\"day\", how=\"left\")\n",
    "\n",
    "# TODO: Define arrival rate and capacity rate (both in picks/hour)\n",
    "arrival_rate = None\n",
    "capacity_rate = None\n",
    "\n",
    "daily2[\"utilization\"] = arrival_rate / capacity_rate\n",
    "\n",
    "# WIP proxy via Little’s Law: throughput (orders/hour) × cycle time (hours)\n",
    "daily2[\"throughput_orders_per_hr\"] = daily2[\"orders\"] / daily2[\"shift_hours\"]\n",
    "daily2[\"wip_proxy_orders\"] = daily2[\"throughput_orders_per_hr\"] * daily2[\"avg_cycle_time_hrs\"]\n",
    "\n",
    "display(daily2[[\"day\",\"orders\",\"avg_cycle_time_hrs\",\"pick_rate_picks_per_lh\",\"utilization\",\"wip_proxy_orders\"]].head(12))\n",
    "daily2[[\"avg_cycle_time_hrs\",\"utilization\",\"wip_proxy_orders\"]].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8fdda9",
   "metadata": {},
   "source": [
    "## Step 3A — Predictive Scenario: Demand Shock (+15%)\n",
    "\n",
    "**Cell ID:** `3A`\n",
    "\n",
    "Apply demand multiplier note impact on utilization under fixed capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8486b6fd",
   "metadata": {
    "tags": [
     "3A"
    ]
   },
   "outputs": [],
   "source": [
    "DEMAND_MULTIPLIER = 1.15\n",
    "\n",
    "daily2[\"arrival_rate_future\"] = daily2[\"total_picks\"] * DEMAND_MULTIPLIER / daily2[\"shift_hours\"]\n",
    "daily2[\"utilization_future\"] = daily2[\"arrival_rate_future\"] / (daily2[\"workers\"] * daily2[\"pick_rate_picks_per_lh\"])\n",
    "\n",
    "display(daily2[[\"day\",\"utilization\",\"utilization_future\"]].head(12))\n",
    "daily2[[\"utilization\",\"utilization_future\"]].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf78393",
   "metadata": {},
   "source": [
    "## Step 3B — Diagnostic Check: Scaling Pattern\n",
    "\n",
    "**Cell ID:** `3B`\n",
    "\n",
    "Compute the ratio utilization_future / utilization and inspect whether it is approximately constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea151898",
   "metadata": {
    "tags": [
     "3B"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scenario B: Non-uniform workload (weekday pattern) + optional single-day shock\n",
    "# Change the weekday multipliers and stress-day controls to observe utilization and staffing impacts downstream.\n",
    "\n",
    "BASE_UPLIFT = 1.25\n",
    "\n",
    "weekday_multiplier = {\n",
    "    0: 1.35,  # Mon\n",
    "    1: 1.10,\n",
    "    2: 1.00,\n",
    "    3: 1.00,\n",
    "    4: 1.20,  # Fri\n",
    "    5: 0.85,\n",
    "    6: 0.75\n",
    "}\n",
    "\n",
    "ENABLE_SINGLE_SHOCK_DAY = True\n",
    "SINGLE_DAY_SHOCK_MULT = 2.20\n",
    "\n",
    "# Calibration constant (keeps ratios intact, makes staffing levels visible)\n",
    "VOLUME_SCALE = 3.50\n",
    "\n",
    "tmp_days = pd.to_datetime(daily2[\"day\"])\n",
    "daily2[\"weekday\"] = tmp_days.dt.weekday\n",
    "\n",
    "# Baseline arrival rate (picks/hour)\n",
    "daily2[\"arrival_rate\"] = daily2[\"total_picks\"] / daily2[\"shift_hours\"]\n",
    "\n",
    "daily2[\"demand_multiplier_B\"] = BASE_UPLIFT * daily2[\"weekday\"].map(weekday_multiplier).astype(float)\n",
    "daily2[\"arrival_rate_future_B\"] = daily2[\"arrival_rate\"] * daily2[\"demand_multiplier_B\"] * VOLUME_SCALE\n",
    "\n",
    "if ENABLE_SINGLE_SHOCK_DAY:\n",
    "    shock_day = tmp_days.iloc[-5]\n",
    "    daily2.loc[tmp_days == shock_day, \"arrival_rate_future_B\"] *= SINGLE_DAY_SHOCK_MULT\n",
    "\n",
    "daily2[\"utilization_future_B\"] = daily2[\"arrival_rate_future_B\"] / (daily2[\"workers\"] * daily2[\"pick_rate_picks_per_lh\"])\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(tmp_days, daily2[\"utilization\"], marker=\"o\", label=\"Baseline utilization\")\n",
    "plt.plot(tmp_days, daily2[\"utilization_future_B\"], marker=\"o\", label=\"Scenario B utilization\")\n",
    "plt.title(\"Utilization — Baseline vs Scenario B (Weekday Peaks + Stress Day)\")\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Utilization\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(daily2[[\"day\",\"weekday\",\"demand_multiplier_B\",\"utilization\",\"utilization_future_B\"]].head(12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5978af",
   "metadata": {},
   "source": [
    "## Step 4A — Prescriptive: Define Target Utilization\n",
    "\n",
    "**Cell ID:** `4A`\n",
    "\n",
    "Target utilization is a policy threshold (e.g., 85%) for stable operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c064609",
   "metadata": {
    "tags": [
     "4A"
    ]
   },
   "outputs": [],
   "source": [
    "TARGET_UTIL = 0.85\n",
    "TARGET_UTIL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6fd47",
   "metadata": {},
   "source": [
    "## Step 4B — Prescriptive: Required Staffing to Hit Target\n",
    "\n",
    "**Cell ID:** `4B`\n",
    "\n",
    "Compute workers needed to keep utilization at/below target under demand shock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10f3e49",
   "metadata": {
    "tags": [
     "4B"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4B) Prescriptive Staffing — Scenario B\n",
    "# Model: demand pressure (arrival_rate_future_B) vs effective execution capacity (workers_effective × pick_rate_effective)\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Policy parameter (decision target)\n",
    "TARGET_UTIL = TARGET_UTIL  # keep as set earlier (e.g., 0.85). Change to test stricter vs looser policy.\n",
    "\n",
    "# Capacity loss (attendance / availability)\n",
    "workers_lost_by_weekday = {\n",
    "    0: 1,  # Mon: absenteeism / onboarding\n",
    "    1: 0,\n",
    "    2: 0,\n",
    "    3: 0,\n",
    "    4: 1,  # Fri: early shift-outs\n",
    "    5: 3,  # Sat: skeleton crew\n",
    "    6: 4   # Sun: minimal crew\n",
    "}\n",
    "\n",
    "# Productivity loss (congestion / fatigue / travel distance)\n",
    "congestion_factor_by_weekday = {\n",
    "    0: 0.70,  # Mon peak congestion\n",
    "    1: 0.85,\n",
    "    2: 1.00,\n",
    "    3: 1.00,\n",
    "    4: 0.75,  # Fri congestion + fatigue\n",
    "    5: 1.05,  # Sat smoother flow\n",
    "    6: 1.10   # Sun light operations\n",
    "}\n",
    "\n",
    "# Stress-test lever (scenario tuning)\n",
    "ENABLE_PEAK_WEEK = True\n",
    "PEAK_WEEK_MULT = 3.0    # adjust upward if required staffing is still below available\n",
    "PEAK_DAYS = 4           # how many last days treated as peak days\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4B1) Effective workers\n",
    "# ---------------------------------------------\n",
    "daily2[\"day\"] = pd.to_datetime(daily2[\"day\"])\n",
    "if \"weekday\" not in daily2.columns:\n",
    "    daily2[\"weekday\"] = daily2[\"day\"].dt.weekday\n",
    "\n",
    "daily2[\"workers_lost\"] = daily2[\"weekday\"].map(workers_lost_by_weekday).astype(float)\n",
    "daily2[\"workers_effective\"] = (daily2[\"workers\"] - daily2[\"workers_lost\"]).clip(lower=1)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4B2) Effective pick rate\n",
    "# ---------------------------------------------\n",
    "daily2[\"congestion_factor\"] = daily2[\"weekday\"].map(congestion_factor_by_weekday).astype(float)\n",
    "daily2[\"pick_rate_effective\"] = (daily2[\"pick_rate_picks_per_lh\"] * daily2[\"congestion_factor\"]).clip(lower=1e-6)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4B3) Peak window multiplier (Scenario B arrival pressure)\n",
    "# ---------------------------------------------\n",
    "df_days = pd.to_datetime(daily2[\"day\"])\n",
    "peak_window = df_days.isin(df_days.sort_values().tail(PEAK_DAYS))\n",
    "\n",
    "if ENABLE_PEAK_WEEK:\n",
    "    daily2.loc[peak_window, \"arrival_rate_future_B\"] *= PEAK_WEEK_MULT\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4B4) Required staffing (raw + rounded) using effective capacity\n",
    "# ---------------------------------------------\n",
    "daily2[\"workers_required_B_raw\"] = daily2[\"arrival_rate_future_B\"] / (TARGET_UTIL * daily2[\"pick_rate_effective\"])\n",
    "daily2[\"workers_required_B\"] = np.ceil(daily2[\"workers_required_B_raw\"])\n",
    "\n",
    "daily2[\"staffing_gap_B_raw\"] = daily2[\"workers_required_B_raw\"] - daily2[\"workers_effective\"]\n",
    "daily2[\"staffing_gap_B\"] = daily2[\"workers_required_B\"] - daily2[\"workers_effective\"]\n",
    "\n",
    "# Total headcount required (same scale as workers_effective)\n",
    "daily2[\"workers_required_total_B\"] = daily2[\"workers_effective\"] + daily2[\"staffing_gap_B\"].clip(lower=0)\n",
    "\n",
    "display(daily2[[\n",
    "    \"day\",\"weekday\",\n",
    "    \"workers\",\"workers_lost\",\"workers_effective\",\n",
    "    \"pick_rate_picks_per_lh\",\"congestion_factor\",\"pick_rate_effective\",\n",
    "    \"arrival_rate_future_B\",\n",
    "    \"workers_required_B_raw\",\"workers_required_B\",\n",
    "    \"staffing_gap_B_raw\",\"staffing_gap_B\",\n",
    "    \"workers_required_total_B\"\n",
    "]].head(12))\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4B5) Dashboards\n",
    "# ---------------------------------------------\n",
    "N = 12\n",
    "dfp = daily2.sort_values(\"day\").tail(N).copy()\n",
    "x = np.arange(len(dfp))\n",
    "\n",
    "# View 1: RAW staffing gap (continuous signal)\n",
    "plt.figure(figsize=(11,4.5))\n",
    "plt.bar(x, dfp[\"staffing_gap_B_raw\"])\n",
    "plt.axhline(0)\n",
    "plt.xticks(x, dfp[\"day\"].dt.strftime(\"%Y-%m-%d\"), rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Staffing Gap (raw workers)\")\n",
    "plt.title(\"Staffing Gap (RAW) — Scenario B vs Effective Staffing\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# View 2: Rounded decision gap (headcount reality)\n",
    "plt.figure(figsize=(11,4.5))\n",
    "plt.bar(x, dfp[\"staffing_gap_B\"])\n",
    "plt.axhline(0)\n",
    "plt.xticks(x, dfp[\"day\"].dt.strftime(\"%Y-%m-%d\"), rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Staffing Gap (rounded workers)\")\n",
    "plt.title(\"Staffing Gap (ROUNDED) — Scenario B vs Effective Staffing\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# View 3: Effective vs Total Required (side-by-side) — rounded decision\n",
    "w = 0.40\n",
    "plt.figure(figsize=(11,4.5))\n",
    "plt.bar(x - w/2, dfp[\"workers_effective\"], width=w, label=\"Effective workers (available)\")\n",
    "plt.bar(x + w/2, dfp[\"workers_required_total_B\"], width=w, label=\"Total workers required (rounded)\")\n",
    "plt.xticks(x, dfp[\"day\"].dt.strftime(\"%Y-%m-%d\"), rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Workers\")\n",
    "plt.title(\"Effective vs Total Required Staffing — Scenario B (Rounded Decision)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Driver view: pick rate degradation (why staffing requirement rises)\n",
    "plt.figure(figsize=(11,3.8))\n",
    "plt.plot(dfp[\"day\"], dfp[\"pick_rate_picks_per_lh\"], marker=\"o\", label=\"Base pick rate\")\n",
    "plt.plot(dfp[\"day\"], dfp[\"pick_rate_effective\"], marker=\"o\", label=\"Effective pick rate (with congestion)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Picks / labor hour\")\n",
    "plt.title(\"Pick Rate Degradation Due to Congestion (Driver of Staffing Requirement)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argos_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
