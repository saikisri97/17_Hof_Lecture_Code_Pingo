{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0209d1c2",
   "metadata": {},
   "source": [
    "# ðŸ§¾ Sales & Demand Planning â€“ Student Notebook (Moderate Difficulty)\n",
    "\n",
    "This notebook walks you through **SEE â†’ TREAT â†’ VERIFY** and basic forecasting on weekly SKU sales data.\n",
    "\n",
    "**You will:**\n",
    "- Inspect and understand the raw dataset (`df_raw`)\n",
    "- Fix calendar and anomaly issues to create `df_treat`\n",
    "- Compute descriptive & diagnostic KPIs\n",
    "- Build a simple moving-average forecast\n",
    "- Make a basic replenishment decision\n",
    "\n",
    "Wherever a transformation is non-trivial, the **exact formula** is written in markdown.\n",
    "You still need to write / uncomment the code, but you don't have to invent the logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c8227",
   "metadata": {},
   "source": [
    "## 0. Imports & Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebbb9aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week</th>\n",
       "      <th>SKU</th>\n",
       "      <th>Sales_Units</th>\n",
       "      <th>Is_Promo</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>SKU_B</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>19.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-19</td>\n",
       "      <td>SKU_A</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>14.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-09-29</td>\n",
       "      <td>SKU_B</td>\n",
       "      <td>-25</td>\n",
       "      <td>1</td>\n",
       "      <td>14.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-21</td>\n",
       "      <td>SKU_B</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>19.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-11-19</td>\n",
       "      <td>SKU_B</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>19.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Week    SKU  Sales_Units  Is_Promo  Price\n",
       "0  2023-05-14  SKU_B          213         0  19.49\n",
       "1  2023-03-19  SKU_A          157         0  14.99\n",
       "2  2024-09-29  SKU_B          -25         1  14.49\n",
       "3  2024-01-21  SKU_B          197         0  19.49\n",
       "4  2023-11-19  SKU_B          187         0  19.49"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: import pandas, numpy, matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# TODO: load df_raw from your CSV\n",
    "# Example:\n",
    "df_raw = pd.read_csv('https://raw.githubusercontent.com/saikisri97/17_Hof_Lecture_Code_Pingo/refs/heads/main/Supply_Chain_Analytics/data/sales_demand_planning.csv')\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0876bdd8",
   "metadata": {},
   "source": [
    "## 0.1 Helper Functions (Already Implemented for You)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3032bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_report(df):\n",
    "    \"\"\"Print count of missing values per column.\"\"\"\n",
    "    print(df.isna().sum())\n",
    "\n",
    "def check_week_structure(df):\n",
    "    \"\"\"Check dtypes and duplicate (Week, SKU) rows.\"\"\"\n",
    "    print('dtypes:\\n', df.dtypes)\n",
    "    if 'Week' in df.columns and 'SKU' in df.columns:\n",
    "        dup = df.duplicated(subset=['Week', 'SKU']).sum()\n",
    "        print('Duplicate (Week, SKU) rows:', dup)\n",
    "\n",
    "def check_anomalies(df):\n",
    "    \"\"\"Check negative sales and extreme promo spikes.\"\"\"\n",
    "    if 'Sales_Units' in df.columns:\n",
    "        print('\\nNegative sales rows:')\n",
    "        print(df[df['Sales_Units'] < 0])\n",
    "    if 'Is_Promo' in df.columns and 'Sales_Units' in df.columns:\n",
    "        print('\\nTop promo spikes:')\n",
    "        print(df[df['Is_Promo'] == 1].sort_values('Sales_Units', ascending=False).head())\n",
    "\n",
    "def step_check(step_name, df):\n",
    "    print(f'--- {step_name} ---')\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60df0720",
   "metadata": {},
   "source": [
    "## STEP 1A â€“ SEE: Structural Checks\n",
    "\n",
    "Use:\n",
    "- `df_raw.dtypes`\n",
    "- `df_raw.shape`\n",
    "- `missing_report(df_raw)`\n",
    "- `check_week_structure(df_raw)`\n",
    "\n",
    "Goal: detect wrong dtypes, missing columns, duplicate (Week, SKU) rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed08d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week            object\n",
      "SKU             object\n",
      "Sales_Units      int64\n",
      "Is_Promo         int64\n",
      "Price          float64\n",
      "dtype: object\n",
      "(206, 5)\n",
      "Week           0\n",
      "SKU            0\n",
      "Sales_Units    0\n",
      "Is_Promo       0\n",
      "Price          0\n",
      "dtype: int64\n",
      "dtypes:\n",
      " Week            object\n",
      "SKU             object\n",
      "Sales_Units      int64\n",
      "Is_Promo         int64\n",
      "Price          float64\n",
      "dtype: object\n",
      "Duplicate (Week, SKU) rows: 4\n"
     ]
    }
   ],
   "source": [
    "# TODO: run structural checks on df_raw\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376db5e1",
   "metadata": {},
   "source": [
    "## STEP 1B â€“ SEE: Anomaly Checks\n",
    "\n",
    "Check for:\n",
    "- Negative `Sales_Units`\n",
    "- Extreme promo spikes\n",
    "- Suspicious `Price` values\n",
    "\n",
    "Useful code patterns:\n",
    "```python\n",
    "df_raw['Sales_Units'].describe()\n",
    "df_raw[df_raw['Sales_Units'] < 0]\n",
    "df_raw[df_raw['Is_Promo'] == 1].sort_values('Sales_Units', ascending=False).head()\n",
    "df_raw['Price'].describe()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3395f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     206.000000\n",
      "mean      189.961165\n",
      "std       104.782229\n",
      "min       -25.000000\n",
      "25%       149.250000\n",
      "50%       182.000000\n",
      "75%       207.000000\n",
      "max      1275.000000\n",
      "Name: Sales_Units, dtype: float64\n",
      "          Week    SKU  Sales_Units  Is_Promo  Price\n",
      "2   2024-09-29  SKU_B          -25         1  14.49\n",
      "30  2024-01-14  SKU_A           -5         0  14.99\n",
      "33  2023-04-23  SKU_B           -5         0  19.49\n",
      "           Week    SKU  Sales_Units  Is_Promo  Price\n",
      "130  2023-10-29  SKU_B         1275         1  14.49\n",
      "194  2024-07-07  SKU_A          975         1   9.99\n",
      "29   2024-03-24  SKU_B          316         1  14.49\n",
      "202  2023-02-19  SKU_B          304         1  14.49\n",
      "123  2024-05-19  SKU_B          303         1  14.49\n",
      "count    206.000000\n",
      "mean      16.468155\n",
      "std        2.854309\n",
      "min        9.990000\n",
      "25%       14.990000\n",
      "50%       14.990000\n",
      "75%       19.490000\n",
      "max       19.490000\n",
      "Name: Price, dtype: float64\n",
      "\n",
      "Negative sales rows:\n",
      "          Week    SKU  Sales_Units  Is_Promo  Price\n",
      "2   2024-09-29  SKU_B          -25         1  14.49\n",
      "30  2024-01-14  SKU_A           -5         0  14.99\n",
      "33  2023-04-23  SKU_B           -5         0  19.49\n",
      "\n",
      "Top promo spikes:\n",
      "           Week    SKU  Sales_Units  Is_Promo  Price\n",
      "130  2023-10-29  SKU_B         1275         1  14.49\n",
      "194  2024-07-07  SKU_A          975         1   9.99\n",
      "29   2024-03-24  SKU_B          316         1  14.49\n",
      "202  2023-02-19  SKU_B          304         1  14.49\n",
      "123  2024-05-19  SKU_B          303         1  14.49\n"
     ]
    }
   ],
   "source": [
    "# TODO: run anomaly checks\n",
    "\n",
    "\n",
    "check_anomalies(df_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2585545",
   "metadata": {},
   "source": [
    "## STEP 1C â€“ SEE Summary\n",
    "Summarise what is wrong with the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57b2fed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Week may be stored as string and can contain invalid patterns\n",
      "- Some (Week, SKU) combinations may be duplicated\n",
      "- There may be negative Sales_Units and big promo spikes\n",
      "- Prices may not always match promo/non-promo expectations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: summarise main data issues you saw\n",
    "see_summary = '''\n",
    "- Week may be stored as string and can contain invalid patterns\n",
    "- Some (Week, SKU) combinations may be duplicated\n",
    "- There may be negative Sales_Units and big promo spikes\n",
    "- Prices may not always match promo/non-promo expectations\n",
    "'''\n",
    "print(see_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c58ccc",
   "metadata": {},
   "source": [
    "## STEP 2A â€“ TREAT: Fix Calendar & Missing Weeks (Guided)\n",
    "\n",
    "Goal: create a clean weekly time series `df_treat`.\n",
    "\n",
    "Follow these **four mini-fixes**, using the exact formulas.\n",
    "\n",
    "### ðŸ§© FIX 1 â€” Convert `Week` â†’ `Week_dt` (datetime)\n",
    "```python\n",
    "df_clean = df_raw.copy()\n",
    "df_clean['Week_dt'] = pd.to_datetime(df_clean['Week'], errors='coerce')\n",
    "df_clean = df_clean.dropna(subset=['Week_dt'])\n",
    "```\n",
    "\n",
    "### ðŸ§© FIX 2 â€” Collapse duplicate (SKU, Week_dt) rows\n",
    "```python\n",
    "df_clean = df_clean.groupby(['SKU','Week_dt'], as_index=False).agg({\n",
    "    'Sales_Units': 'sum',\n",
    "    'Is_Promo': 'max',\n",
    "    'Price': 'mean'\n",
    "})\n",
    "```\n",
    "\n",
    "### ðŸ§© FIX 3 â€” Create the full weekly calendar\n",
    "```python\n",
    "full_weeks = pd.date_range(\n",
    "    df_clean['Week_dt'].min().normalize(),\n",
    "    df_clean['Week_dt'].max().normalize(),\n",
    "    freq='W-SUN'\n",
    ")\n",
    "```\n",
    "\n",
    "### ðŸ§© FIX 4 â€” Reindex each SKU to the full calendar\n",
    "```python\n",
    "df_list = []\n",
    "for sku, sub in df_clean.groupby('SKU'):\n",
    "    sub = sub.set_index('Week_dt').reindex(full_weeks)\n",
    "    sub['SKU'] = sku\n",
    "    df_list.append(sub)\n",
    "\n",
    "df_treat = pd.concat(df_list).reset_index().rename(columns={'index':'Week_dt'})\n",
    "df_treat['Week'] = df_treat['Week_dt'].dt.strftime('%Y-%m-%d')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b9d2b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2A_calendar_fixed ---\n",
      "     Week_dt    SKU  Sales_Units  Is_Promo  Price        Week\n",
      "0 2023-01-01  SKU_A        145.0       0.0  14.99  2023-01-01\n",
      "1 2023-01-08  SKU_A        141.0       0.0  14.99  2023-01-08\n",
      "2 2023-01-15  SKU_A        210.0       1.0   9.99  2023-01-15\n",
      "3 2023-01-22  SKU_A        163.0       0.0  14.99  2023-01-22\n",
      "4 2023-01-29  SKU_A        148.0       0.0  14.99  2023-01-29\n"
     ]
    }
   ],
   "source": [
    "# STEP 2A â€“ implement the 4 fixes using the formulas above\n",
    "# 1) Week â†’ Week_dt\n",
    "df_clean = df_raw.copy()\n",
    "df_clean['Week_dt'] = \n",
    "df_clean = df_clean.dropna(subset=['Week_dt'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df894bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Collapse duplicates per SKU & Week_dt\n",
    "df_clean = df_clean.groupby(['SKU','Week_dt'], as_index=False).agg({\n",
    "    'Sales_Units': '',\n",
    "   'Is_Promo': '',\n",
    "    'Price': ''\n",
    " })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f572b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Build full weekly calendar\n",
    "full_weeks = pd.date_range(\n",
    "  \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b850b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Reindex per SKU\n",
    "df_list = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd63be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treat = pd.concat(df_list).reset_index().rename(columns={'index':'Week_dt'})\n",
    "df_treat['Week'] = df_treat['Week_dt'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "step_check('2A_calendar_fixed', df_treat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1959ffb4",
   "metadata": {},
   "source": [
    "## STEP 2B â€“ TREAT: Fix Anomalies\n",
    "\n",
    "Now clean value problems in `df_treat`.\n",
    "\n",
    "### ðŸ§© FIX A â€” Replace negative sales with 0\n",
    "```python\n",
    "df_treat['Sales_Units'] = df_treat['Sales_Units'].fillna(0)\n",
    "df_treat.loc[df_treat['Sales_Units'] < 0, 'Sales_Units'] = 0\n",
    "```\n",
    "\n",
    "### ðŸ§© FIX B â€” Cap extreme promo spikes per SKU\n",
    "```python\n",
    "for sku, sub in df_treat.groupby('SKU'):\n",
    "    mask = (df_treat['SKU'] == sku) & (df_treat['Is_Promo'] == 1)\n",
    "    cap = sub[sub['Is_Promo'] == 1]['Sales_Units'].quantile(0.99)\n",
    "    df_treat.loc[mask & (df_treat['Sales_Units'] > cap), 'Sales_Units'] = cap\n",
    "```\n",
    "\n",
    "### ðŸ§© FIX C â€” Fix prices using forward/backward fill\n",
    "```python\n",
    "df_treat['Price'] = df_treat.groupby('SKU')['Price'].ffill().bfill()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca80e9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2B_anomalies_fixed ---\n",
      "     Week_dt    SKU  Sales_Units  Is_Promo  Price        Week\n",
      "0 2023-01-01  SKU_A        145.0       0.0  14.99  2023-01-01\n",
      "1 2023-01-08  SKU_A        141.0       0.0  14.99  2023-01-08\n",
      "2 2023-01-15  SKU_A        210.0       1.0   9.99  2023-01-15\n",
      "3 2023-01-22  SKU_A        163.0       0.0  14.99  2023-01-22\n",
      "4 2023-01-29  SKU_A        148.0       0.0  14.99  2023-01-29\n"
     ]
    }
   ],
   "source": [
    "# STEP 2B â€“ apply anomaly fixes from the formulas above\n",
    "df_treat['Sales_Units'] = \n",
    "\n",
    "for sku, sub in df_treat.groupby('SKU'):\n",
    "     \n",
    "\n",
    "df_treat['Price'] = df_treat.groupby('SKU')['Price'].ffill().bfill()\n",
    "\n",
    "\n",
    "step_check('2B_anomalies_fixed', df_treat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d702176a",
   "metadata": {},
   "source": [
    "## STEP 2C â€“ VERIFY after TREAT\n",
    "\n",
    "Re-run your SEE helpers on `df_treat`.\n",
    "\n",
    "Use:\n",
    "```python\n",
    "missing_report(df_treat)\n",
    "check_week_structure(df_treat)\n",
    "check_anomalies(df_treat)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5256a79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week_dt        0\n",
      "SKU            0\n",
      "Sales_Units    0\n",
      "Is_Promo       7\n",
      "Price          0\n",
      "Week           0\n",
      "dtype: int64\n",
      "dtypes:\n",
      " Week_dt        datetime64[ns]\n",
      "SKU                    object\n",
      "Sales_Units           float64\n",
      "Is_Promo              float64\n",
      "Price                 float64\n",
      "Week                   object\n",
      "dtype: object\n",
      "Duplicate (Week, SKU) rows: 0\n",
      "\n",
      "Negative sales rows:\n",
      "Empty DataFrame\n",
      "Columns: [Week_dt, SKU, Sales_Units, Is_Promo, Price, Week]\n",
      "Index: []\n",
      "\n",
      "Top promo spikes:\n",
      "       Week_dt    SKU  Sales_Units  Is_Promo  Price        Week\n",
      "147 2023-10-29  SKU_B      1140.74       1.0  14.49  2023-10-29\n",
      "79  2024-07-07  SKU_A       874.34       1.0   9.99  2024-07-07\n",
      "168 2024-03-24  SKU_B       316.00       1.0  14.49  2024-03-24\n",
      "111 2023-02-19  SKU_B       304.00       1.0  14.49  2023-02-19\n",
      "176 2024-05-19  SKU_B       303.00       1.0  14.49  2024-05-19\n"
     ]
    }
   ],
   "source": [
    "# STEP 2C â€“ VERIFY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c011c3",
   "metadata": {},
   "source": [
    "## STEP 3A â€“ Descriptive Analytics\n",
    "\n",
    "Compute basic stats per SKU:\n",
    "```python\n",
    "df_treat.groupby('SKU')['Sales_Units'].agg(['mean','std','min','max'])\n",
    "```\n",
    "\n",
    "Compare promo vs non-promo averages:\n",
    "```python\n",
    "promo_vs_non = df_treat.groupby(['SKU','Is_Promo'])['Sales_Units'].mean().unstack()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea9bd5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             mean         std  min      max\n",
      "SKU                                        \n",
      "SKU_A  163.955192   80.474425  0.0   874.34\n",
      "SKU_B  208.324423  112.392169  0.0  1140.74\n",
      "Is_Promo         0.0      1.0\n",
      "SKU                          \n",
      "SKU_A     151.793103  256.356\n",
      "SKU_B     201.047619  318.516\n"
     ]
    }
   ],
   "source": [
    "# TODO: descriptive KPIs\n",
    "desc_stats = df_treat.groupby('SKU')['Sales_Units'].agg\n",
    "print(desc_stats)\n",
    "\n",
    "promo_vs_non = df_treat.groupby(['SKU','Is_Promo'])\n",
    "print(promo_vs_non)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4104977",
   "metadata": {},
   "source": [
    "## STEP 3B â€“ Diagnostic Analytics\n",
    "\n",
    "Compute promo uplift (%):\n",
    "```python\n",
    "uplift = (promo_vs_non[1] - promo_vs_non[0]) / promo_vs_non[0] * 100\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c709fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promo uplift (%):\n",
      "SKU\n",
      "SKU_A    68.885143\n",
      "SKU_B    58.428138\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# TODO: compute promo uplift\n",
    "uplift = \n",
    "print('Promo uplift (%):')\n",
    "print(uplift)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4718119",
   "metadata": {},
   "source": [
    "## STEP 3C â€“ Predictive Analytics (3-week Moving Average)\n",
    "\n",
    "Compute a 3-week simple moving average per SKU:\n",
    "```python\n",
    "df_ma_list = []\n",
    "for sku, sub in df_treat.groupby('SKU'):\n",
    "    sub = sub.sort_values('Week_dt').copy()\n",
    "    sub['SMA_3'] = sub['Sales_Units'].rolling(window=3).mean()\n",
    "    df_ma_list.append(sub)\n",
    "\n",
    "df_forecast_all = pd.concat(df_ma_list)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec2bd3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKU SKU_A: MAPE=11.4%, Bias=-0.17\n",
      "SKU SKU_B: MAPE=17.6%, Bias=-3.77\n"
     ]
    }
   ],
   "source": [
    "df_ma_list = []\n",
    "for sku, sub in df_treat.groupby('SKU'):\n",
    "\n",
    "\n",
    "df_forecast_all = pd.concat(df_ma_list)\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def bias(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(y_pred - y_true)\n",
    "\n",
    "for sku, sub in df_forecast_all.groupby('SKU'):\n",
    "    eval_sub = sub.dropna(subset=['SMA_3']).tail(20)\n",
    "    m = mape(eval_sub['Sales_Units'], eval_sub['SMA_3'])\n",
    "    b = bias(eval_sub['Sales_Units'], eval_sub['SMA_3'])\n",
    "    print(f'SKU {sku}: MAPE={m:.1f}%, Bias={b:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9de050",
   "metadata": {},
   "source": [
    "## STEP 4A â€“ Forecast Next Week\n",
    "\n",
    "Use the latest SMA_3 value for each SKU as next-week forecast:\n",
    "```python\n",
    "next_week_forecast = {}\n",
    "for sku, sub in df_forecast_all.groupby('SKU'):\n",
    "    latest = sub.dropna(subset=['SMA_3']).sort_values('Week_dt').iloc[-1]\n",
    "    next_week_forecast[sku] = latest['SMA_3']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf9ee817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SKU_A': 150.33333333333334, 'SKU_B': 203.33333333333334}\n"
     ]
    }
   ],
   "source": [
    "# TODO: create next_week_forecast dict\n",
    "next_week_forecast = {}\n",
    "for sku, sub in df_forecast_all.groupby('SKU'):\n",
    "\n",
    "print(next_week_forecast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68580e49",
   "metadata": {},
   "source": [
    "## STEP 4B â€“ Prescriptive: Replenishment Decision\n",
    "\n",
    "Assume:\n",
    "- Service level factor: `z = 1.65` (â‰ˆ95%)\n",
    "- Safety stock formula: `Safety_Stock = z * weekly_std`\n",
    "- Reorder point: `ROP = forecast + Safety_Stock`\n",
    "- Use a simple assumption for `current_inventory` (e.g. equal to forecast).\n",
    "\n",
    "Example pattern:\n",
    "```python\n",
    "service_level_z = 1.65\n",
    "replenishment_plan = {}\n",
    "for sku, sub in df_treat.groupby('SKU'):\n",
    "    sub = sub.sort_values('Week_dt')\n",
    "    weekly_std = sub['Sales_Units'].tail(26).std()\n",
    "    forecast = next_week_forecast[sku]\n",
    "    safety_stock = service_level_z * weekly_std\n",
    "    rop = forecast + safety_stock\n",
    "    current_inventory = forecast\n",
    "    order_qty = max(0, rop - current_inventory)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a28b1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SKU_A': {'forecast': 150.33333333333334,\n",
       "  'weekly_std': 144.2608863776371,\n",
       "  'safety_stock': 238.0304625231012,\n",
       "  'ROP': 388.36379585643454,\n",
       "  'current_inventory': 150.33333333333334,\n",
       "  'order_quantity': 238.0304625231012},\n",
       " 'SKU_B': {'forecast': 203.33333333333334,\n",
       "  'weekly_std': 77.8089672506671,\n",
       "  'safety_stock': 128.3847959636007,\n",
       "  'ROP': 331.7181292969341,\n",
       "  'current_inventory': 203.33333333333334,\n",
       "  'order_quantity': 128.38479596360074}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: compute safety stock, ROP, and order quantity per SKU\n",
    "service_level_z = 1.65\n",
    "replenishment_plan = {}\n",
    "\n",
    "for sku, sub in df_treat.groupby('SKU'):\n",
    "\n",
    "    replenishment_plan[sku] = {\n",
    "        'forecast': forecast,\n",
    "        'weekly_std': weekly_std,\n",
    "        'safety_stock': safety_stock,\n",
    "        'ROP': rop,\n",
    "        'current_inventory': current_inventory,\n",
    "        'order_quantity': order_qty,\n",
    "       \n",
    "    }\n",
    "replenishment_plan\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argos_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
